{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import pycuber\n",
    "import random\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "# plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так выглядит его описание и вот так можно его преобразовать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube = pycuber.Cube()\n",
    "print(str(cube))\n",
    "print('')\n",
    "\n",
    "# bad\n",
    "print(str(cube).replace(' ', '').replace('\\n', '').replace('[', '').replace(']', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но лучше по очереди каждую стороны."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_state = str(cube).replace(' ', '').replace('\\n', '').replace('[', '').replace(']', '')\n",
    "\n",
    "# better\n",
    "state = (\n",
    "    raw_state[:9] +\n",
    "    raw_state[9:12] + raw_state[21:24] + raw_state[33:36] +\n",
    "    raw_state[12:15] + raw_state[24:27] + raw_state[36:39] +\n",
    "    raw_state[15:18] + raw_state[27:30] + raw_state[39:42] +\n",
    "    raw_state[18:21] + raw_state[30:33] + raw_state[42:45] +\n",
    "    raw_state[-9:]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выучим один раз one-hot для этого кубика."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder()\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe.fit(np.array(list(state)).reshape((-1, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для извлечения состояния, описывающего кубик."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_state(cube):\n",
    "    raw_state = str(cube).replace(' ', '').replace('\\n', '').replace('[', '').replace(']', '')\n",
    "\n",
    "    state = (\n",
    "        raw_state[:9] +\n",
    "        raw_state[9:12] + raw_state[21:24] + raw_state[33:36] +\n",
    "        raw_state[12:15] + raw_state[24:27] + raw_state[36:39] +\n",
    "        raw_state[15:18] + raw_state[27:30] + raw_state[39:42] +\n",
    "        raw_state[18:21] + raw_state[30:33] + raw_state[42:45] +\n",
    "        raw_state[-9:])\n",
    "    \n",
    "    return np.array(ohe.transform(np.array(list(state)).reshape((-1, 1))).todense(), dtype=np.float32).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Список возможных действий."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [\"L\", \"L'\", \"R\", \"R'\", \"U\", \"U'\", \"D\", \"D'\", \"F\", \"F'\", \"B\", \"B'\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_action = len(actions)\n",
    "n_space = len(extract_state(cube))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_state = extract_state(pycuber.Cube())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube = pycuber.Cube()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for action in np.random.randint(0, n_action, 10):\n",
    "#     cube.perform_step(pycuber.Step(actions[action]))\n",
    "#     state = extract_state(cube)\n",
    "#     print np.sum(final_state == state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Среда в формате gym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CubeEnv(gym.Env):\n",
    "    def __init__(self, min_steps, max_steps):\n",
    "        self.action_space = spaces.Discrete(n_action)\n",
    "        self.observation_space = spaces.Discrete(n_space)    \n",
    "        self._cube = None\n",
    "        self._min_steps = min_steps\n",
    "        self._max_steps = max_steps\n",
    "        self._quality = 0\n",
    "        \n",
    "        self.seed()\n",
    "        self.reset()\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        np.random.seed(seed)\n",
    "        return seed\n",
    "\n",
    "    def step(self, action):\n",
    "        self._cube.perform_step(pycuber.Step(actions[action]))\n",
    "        observation = extract_state(self._cube)\n",
    "        reward = self.calc_reward(observation)\n",
    "\n",
    "        return observation, reward, all(observation == final_state), {}\n",
    "    \n",
    "    def calc_reward(self, observation):\n",
    "#         global steps_done\n",
    "#         alpha = min(max(0, (steps_done - 10000) / REWARD_CHANGE_STEPS), 1)\n",
    "#         new_quality = (\n",
    "#             (1 - alpha) * np.mean(observation == final_state) +\n",
    "#             alpha * int(all(observation == final_state))) * 100\n",
    "        new_quality = np.mean(observation == final_state) * 100\n",
    "        \n",
    "        reward = new_quality - self._quality\n",
    "        self._quality = new_quality\n",
    "        return reward\n",
    "\n",
    "    def reset(self):\n",
    "        self._cube = pycuber.Cube()\n",
    "        random_actions = np.random.randint(\n",
    "            0, n_action,\n",
    "            size=np.random.randint(self._min_steps, self._max_steps + 1))\n",
    "        self._cube.perform_algo(pycuber.Formula(list(map(lambda x: actions[x], random_actions))))\n",
    "        observation = extract_state(self._cube)\n",
    "        self.calc_reward(observation)\n",
    "        \n",
    "        return observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_space, 150)\n",
    "        self.layer2 = nn.Linear(150, 100)\n",
    "        self.layer3 = nn.Linear(100, 50)\n",
    "        self.head = nn.Linear(50, n_action)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.relu(self.layer3(x))\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "\n",
    "#     epsilon greedy\n",
    "#     global steps_done\n",
    "#     sample = random.random()\n",
    "#     eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "#         np.exp(-1. * steps_done / EPS_DECAY)\n",
    "    \n",
    "#     output = policy_net(state)\n",
    "#     probs = np.exp((output[0].detach().numpy() + EPS) / TAU)\n",
    "#     probs /= probs.sum()\n",
    "    \n",
    "#     if sample > eps_threshold:\n",
    "#         with torch.no_grad():\n",
    "#             return output.max(1)[1].view(1, 1), probs\n",
    "#     else:\n",
    "#         return torch.tensor([[random.randrange(n_action)]], dtype=torch.long), probs\n",
    "    \n",
    "    # boltzman\n",
    "    categorical = Categorical(logits=policy_net(state).detach())\n",
    "    \n",
    "#     probs = np.exp((policy_net(state)[0].detach().numpy() + EPS) / TAU)\n",
    "#     probs /= probs.sum()\n",
    "#     action = np.random.choice(np.arange(n_action), p=probs)\n",
    "    \n",
    "#     return torch.tensor([[action]], dtype=torch.long), probs\n",
    "    return categorical.sample(), categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(env, policy_net, n_episodes, max_episodes_length):\n",
    "    test_reward = []\n",
    "    test_entropy = []\n",
    "    n_success_episodes = 0\n",
    "    \n",
    "    for i_episode in range(n_episodes):\n",
    "        episode_reward = 0\n",
    "        state = torch.tensor([env.reset()])\n",
    "        \n",
    "        for t in range(max_episodes_length):\n",
    "            output = policy_net(state).detach()\n",
    "            action = output.max(1)[1]\n",
    "#             probs = output.exp()[0].detach().numpy() + EPS\n",
    "#             probs /= probs.sum()\n",
    "#             test_entropy.append(np.sum(- probs * np.log(probs + EPS)))\n",
    "            categorical = Categorical(logits=output)\n",
    "            test_entropy.append(float(categorical.entropy()))\n",
    "\n",
    "            state, reward, done, _ = env.step(action.item())\n",
    "            state = torch.tensor([state])\n",
    "            episode_reward += reward\n",
    "            if done:\n",
    "                n_success_episodes += 1\n",
    "                break\n",
    "\n",
    "        test_reward.append(episode_reward)\n",
    "    \n",
    "    return float(n_success_episodes) / n_episodes, np.mean(test_reward), np.mean(test_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    \n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), dtype=torch.uint8)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    \n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch.view(-1, 1))\n",
    "\n",
    "    # compute V(s_{t+1}) for all next states.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    \n",
    "    # compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    loss = F.mse_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "    \n",
    "    weights = policy_net.state_dict()\n",
    "    weights_norm = np.mean([np.mean(w.numpy() ** 2) for _, w in weights.items()])\n",
    "    writer.add_scalar('network/norm_weights', weights_norm, steps_done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(learn_env, test_env, num_episodes):\n",
    "    global steps_done\n",
    "    global episode_durations\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        state = torch.tensor([learn_env.reset()])\n",
    "\n",
    "        for t in count():\n",
    "            action, _ = select_action(state)\n",
    "            next_state, reward, done, _ = learn_env.step(action.item())\n",
    "            next_state = torch.tensor([next_state])\n",
    "            reward = torch.tensor([reward], dtype=torch.float)\n",
    "\n",
    "            if done:\n",
    "                next_state = None\n",
    "\n",
    "            memory.push(state, action, next_state, reward)\n",
    "            state = next_state\n",
    "\n",
    "            if done or t == max_train_episode_length - 1:\n",
    "                episode_durations.append(t + 1)\n",
    "                break\n",
    "\n",
    "        optimize_model()\n",
    "        if steps_done % TARGET_UPDATE == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        if steps_done % TEST_PERIOD == 0:\n",
    "            success_rate, avg_reward, avg_entropy = test_agent(test_env, policy_net, n_episodes=100, \n",
    "                                                               max_episodes_length=100)\n",
    "            print(steps_done, success_rate, avg_reward)\n",
    "            \n",
    "            writer.add_scalar('cube/success_rate', success_rate, steps_done)\n",
    "            writer.add_scalar('cube/avg_reward', avg_reward, steps_done)\n",
    "            writer.add_scalar('policy/avg_entropy', avg_entropy, steps_done)\n",
    "#             pickle.dump(policy_net.state_dict(), open('pytorch/weights_{}.pkl'.format(steps_done), 'wb'))\n",
    "            \n",
    "        steps_done += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-100\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.9\n",
    "TAU = 1.0\n",
    "\n",
    "TARGET_UPDATE = 500\n",
    "TEST_PERIOD = 100\n",
    "\n",
    "# EPS_START = 0.9\n",
    "# EPS_END = 0.05\n",
    "# EPS_DECAY = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = DQN()\n",
    "target_net = DQN()\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters())\n",
    "memory = ReplayMemory(1000000)\n",
    "steps_done = 0\n",
    "\n",
    "writer = SummaryWriter(log_dir='logs/random_learn_5fix-5-10random_test_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn_env = CubeEnv(min_steps=5, max_steps=5)\n",
    "test_env = CubeEnv(min_steps=10, max_steps=10)\n",
    "\n",
    "episode_durations = []\n",
    "max_train_episode_length = 50\n",
    "\n",
    "train(learn_env=learn_env, test_env=test_env, num_episodes=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn_env = CubeEnv(min_steps=5, max_steps=10)\n",
    "test_env = CubeEnv(min_steps=10, max_steps=10)\n",
    "\n",
    "episode_durations = []\n",
    "max_train_episode_length = 50\n",
    "\n",
    "train(learn_env=learn_env, test_env=test_env, num_episodes=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success_rate, avg_reward, avg_entropy = \\\n",
    "    test_agent(test_env, policy_net, n_episodes=1000, max_episodes_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success_rate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
