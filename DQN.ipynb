{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%pylab inline\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "from itertools import count\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import pycuber\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import cube\n",
    "import dqn\n",
    "import solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(env, policy_net, n_episodes, max_episodes_length):\n",
    "    test_reward = []\n",
    "    test_entropy = []\n",
    "    n_success_episodes = 0\n",
    "    \n",
    "    for i_episode in range(n_episodes):\n",
    "        episode_reward = 0\n",
    "        state = env.reset().view(1, -1)\n",
    "        \n",
    "        for t in range(max_episodes_length):\n",
    "            output = policy_net(state).detach()\n",
    "            action = output.max(1)[1]\n",
    "            categorical = Categorical(logits=output)\n",
    "            test_entropy.append(float(categorical.entropy()))\n",
    "\n",
    "            state, reward, done, _ = env.step(action.item())\n",
    "            state = state.view(1, -1)\n",
    "            episode_reward += reward\n",
    "            if done:\n",
    "                n_success_episodes += 1\n",
    "                break\n",
    "\n",
    "        test_reward.append(episode_reward)\n",
    "    \n",
    "    return (\n",
    "        float(n_success_episodes) / n_episodes,\n",
    "        np.mean(test_reward),\n",
    "        np.mean(test_entropy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    \n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = dqn.Transition(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(\n",
    "        tuple(map(lambda s: s is not None, batch.next_state)),\n",
    "        dtype=torch.uint8)\n",
    "    non_final_next_states = torch.cat(\n",
    "        [s for s in batch.next_state if s is not None])\n",
    "    \n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken\n",
    "    state_action_values = policy_net(state_batch).gather(\n",
    "        1, action_batch.view(-1, 1))\n",
    "\n",
    "    # compute V(s_{t+1}) for all next states.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE)\n",
    "    next_state_values[non_final_mask] = target_net(\n",
    "        non_final_next_states).max(1)[0].detach()\n",
    "    \n",
    "    # compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    loss = F.mse_loss(\n",
    "        state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "    \n",
    "    weights = policy_net.state_dict()\n",
    "    weights_norm = np.mean(\n",
    "        [np.mean(w.numpy() ** 2) for _, w in weights.items()])\n",
    "    writer.add_scalar('network/norm_weights', weights_norm, steps_done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(learn_env, test_env, num_episodes):\n",
    "    global steps_done\n",
    "    global episode_durations\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        state = learn_env.reset().view(1, -1)\n",
    "        \n",
    "        for t in count():\n",
    "            action = policy_net.sample_action(state)\n",
    "            next_state, reward, done, _ = learn_env.step(action.item())\n",
    "            next_state = next_state.view(1, -1)\n",
    "            reward = torch.FloatTensor([reward])\n",
    "\n",
    "            if done:\n",
    "                next_state = None\n",
    "\n",
    "            memory.push(state, action, next_state, reward)\n",
    "            state = next_state\n",
    "\n",
    "            if done or t == max_train_episode_length - 1:\n",
    "                episode_durations.append(t + 1)\n",
    "                break\n",
    "\n",
    "        optimize_model()\n",
    "        if steps_done % TARGET_UPDATE == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        if steps_done % TEST_PERIOD == 0:\n",
    "            success_rate, avg_reward, avg_entropy = \\\n",
    "                test_agent(\n",
    "                    test_env, policy_net, n_episodes=100,\n",
    "                    max_episodes_length=100)\n",
    "            print(steps_done, success_rate, avg_reward)\n",
    "            \n",
    "            writer.add_scalar('cube/success_rate', success_rate, steps_done)\n",
    "            writer.add_scalar('cube/avg_reward', avg_reward, steps_done)\n",
    "            writer.add_scalar('policy/avg_entropy', avg_entropy, steps_done)\n",
    "            \n",
    "        if steps_done and steps_done % SAVE_PERIOD == 0:\n",
    "            pickle.dump(policy_net.state_dict(), open(\n",
    "                'weights/dqn_steps_10_mean_100_gamma_0.9_batch_512_episodes_{}.pkl'.format(steps_done), 'wb'))\n",
    "            \n",
    "        steps_done += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-100\n",
    "BATCH_SIZE = 512\n",
    "GAMMA = 0.9\n",
    "TAU = 1.0\n",
    "\n",
    "TARGET_UPDATE = 500\n",
    "TEST_PERIOD = 1000\n",
    "SAVE_PERIOD = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = dqn.DQN(n_space=cube.N_SPACE, n_action=cube.N_ACTION)\n",
    "target_net = dqn.DQN(n_space=cube.N_SPACE, n_action=cube.N_ACTION)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters())\n",
    "memory = dqn.ReplayMemory(1000000)\n",
    "steps_done = 0\n",
    "\n",
    "writer = SummaryWriter(log_dir='logs/dqn_steps_10_scale_100_batch_512')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learn_env = cube.CubeEnv(steps=10, reward_scale=100)\n",
    "test_env = cube.CubeEnv(steps=5)\n",
    "\n",
    "episode_durations = []\n",
    "max_train_episode_length = 50\n",
    "\n",
    "train(learn_env=learn_env, test_env=test_env, num_episodes=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(\n",
    "    policy_net.state_dict(),\n",
    "    open('weights/dqn_steps_10_mean_100_gamma_0.9_batch_512_episodes_100000.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = cube.CubeEnv(steps=5)\n",
    "\n",
    "success_rate, avg_reward, avg_entropy = \\\n",
    "    test_agent(test_env, policy_net, n_episodes=1000, max_episodes_length=100)\n",
    "\n",
    "print(success_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = cube.CubeEnv(steps=10)\n",
    "\n",
    "success_rate, avg_reward, avg_entropy = \\\n",
    "    test_agent(test_env, policy_net, n_episodes=1000, max_episodes_length=100)\n",
    "\n",
    "print(success_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
